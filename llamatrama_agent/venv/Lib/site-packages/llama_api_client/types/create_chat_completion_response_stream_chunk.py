# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import List, Union, Optional
from typing_extensions import Literal, Annotated, TypeAlias

from .._utils import PropertyInfo
from .._models import BaseModel

__all__ = [
    "CreateChatCompletionResponseStreamChunk",
    "Event",
    "EventDelta",
    "EventDeltaTextDelta",
    "EventDeltaToolCallDelta",
    "EventDeltaToolCallDeltaFunction",
    "EventMetric",
]


class EventDeltaTextDelta(BaseModel):
    text: str

    type: Literal["text"]


class EventDeltaToolCallDeltaFunction(BaseModel):
    arguments: Optional[str] = None
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: Optional[str] = None
    """The name of the function to call."""


class EventDeltaToolCallDelta(BaseModel):
    function: EventDeltaToolCallDeltaFunction

    type: Literal["tool_call"]

    id: Optional[str] = None
    """The ID of the tool call."""


EventDelta: TypeAlias = Annotated[
    Union[EventDeltaTextDelta, EventDeltaToolCallDelta], PropertyInfo(discriminator="type")
]


class EventMetric(BaseModel):
    metric: str

    value: float

    unit: Optional[str] = None


class Event(BaseModel):
    delta: EventDelta
    """Content generated since last event.

    This can be one or more tokens, or a tool call.
    """

    event_type: Literal["start", "complete", "progress", "metrics"]
    """Type of the event"""

    metrics: Optional[List[EventMetric]] = None

    stop_reason: Optional[Literal["stop", "tool_calls", "length"]] = None
    """The reason why we stopped.

    Options are: - "stop": The model reached a natural stopping point. -
    "tool_calls": The model finished generating and invoked a tool call. - "length":
    The model reached the maxinum number of tokens specified in the request.
    """


class CreateChatCompletionResponseStreamChunk(BaseModel):
    event: Event
    """The event containing the new content"""

    id: Optional[str] = None
    """The unique identifier of the chat completion request."""
